---
title             : "Consistency and variability in the development of children's drawings between two countries"
shorttitle        : "Development of drawing"

author: 
  - name          : "Bria Long"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Jane Stanford Way, Stanford CA 94305"
    email         : "bria@stanford.edu"
  - name          : "Ying Wang"
    affiliation   : "2"
  - name          : "Stella Christie"
    affiliation   : "2"
  - name          : "Michael C. Frank"
    affiliation   : "1"
  - name          : "Judith E. Fan"
    affiliation   : "3"

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "Tsinghua University"
  - id            : "3"
    institution   : "University of California, San Diego"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
 Childen's drawings of common object categories become dramatically more recognziable across development. What are the major factors that explain these developmental changes? Here, we examined the degree to which these changes in recognzibility vary across different drawing tasks (i.e. drawing from observation vs. from memory), geographical locations,  and children's tracing abilities. To do so, we collecting digital drawings of object categories (e.g., cat, airplane) from 4-9 year-olds (N=253) who were recruited near San Jose, USA and Beijing, China. Overall, we show broad consistency in these developmental trajectories across object categories, drawing tasks, and these two geographical locations, and we find that children's tracings abilites are good predictors of the recognizability of the drawings that they produce. In addition, we find that children recruited near Beijing, China produced overal more recognizable drawings but showed similar tracing abilities to childen recruited near San Jose, CA.  Overall, this work suggests that the developmental trajectory of children's drawings are remarkably consistent and not easily explainable by changes in their visuomotor control. 
 
keywords          : "children's drawings, visual production, tracing, object recognition, visuomotor control"
wordcount         : "X"

bibliography      : ["references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(tidyverse)
library(here)
library(assertthat)
library(langcog)
library(ggthemes)
library(knitr)
library(lmerTest)
library(lme4)
library(xtable)
```

As humans, we have many powerful tools to externalize what we know, including language and gesture. 
One tool that has been transformative for human cognition and culture is graphical representation, which allows people to encode their thoughts in a visible, durable format. 
Drawing is an important case study in graphical representation, being a technique that dates back 60,000 years [@hoffmann2018u], well before the emergence of symbolic writing systems, and is practiced in many cultures.

In modern times, drawings are produced prolifically by children from an early age. 
Figurative drawings have long provided inspiration for scientists investigating children's emerging cognitive abilities [@minsky1972artificial], and accordingly a long history of work has examined changes in children's drawings across development [@piaget1929child; @kellogg1969analyzing; @karmiloff1990constraints; @fury1997children].
Indeed, there appear to be dramatic changes in how children encode diagnostic visual information in their drawings across age; younger children (4-5 years) tend to include fewer cues in their drawings to differentiate between target concepts (e.g., \textit{adult} vs. \textit{child}) than older children, who enrich their drawings with more diagnostic part [@sitton1992drawing] and relational [@light1983effects] information. 
Overall, younger children produce drawings of object categories that are less diagnostic of the categories they are trying to depict [@long2018drawings]. 
 
What drives these dramatic changes in children's drawings across development? 
A common view is that these changes are driven primarily by children's increasing ability to plan and control their motor movements [@freeman1987current; @rehrig2018does]. 
While such changes in visuomotor control are clearly important, this view fails to account for other important constraints, such as how well children are able to access previously acquired semantic knowledge about each target concept and maintain this information in working memory during drawing production.
% how recognizable drawing is a complex skill that integrates many different aspects of cognition. 
% For example, in order to "draw a [rabbit]", a child has to access their mental representation of a rabbit, choose the particular visual features needed to convey this visual concept, and then maintain this representation and goal in mind as they plan and produce a series of motor movements. 
Thus, drawing is a skill that likely also relies on children's evolving perceptual category representations [@long2018drawings; @natu2016development; @dekker2011dorsal] as well as their increasing working memory capacity [@pailian2016visual].

One idea is that a principal reason younger children produce less recognizable drawings is because they simply have more difficulty recalling the relevant perceptual features of different categories: that is, when asked to "draw a [rabbit]", they may struggle to conjure up the relevant perceptual details and then hold in mind what rabbits tend to look like. 
On this account, providing children with additional perceptual information about different categories -- for example, canonical photographs of typical exemplars -- could help them improve their drawings of these categories, as it does with adults [@fan2020relating]. 
However, prior work also suggests that younger children tend to draw what they know about objects rather than integrate the information in their immediate perceptual experience.  
For example, when asked to draw from observation, younger children tend to include features that are not visible from their vantage point, yet are diagnostic of category membership (e.g., a handle on a \textit{mug}) [@bremmer1984prior; @barrett1976symbolism], and only omit these features later in development. 
Similarly, young children will insist that their nearly identical drawings of different concepts (e.g., balloon and person) unambiguously refer to different things [@bloom1998intention]. 
Thus, an alternative possibility is that only older children may be able to produce more recognizable drawings when provided with canonical exemplars of different categories. 
On this account, changes in children's drawings from memory across age may be largely due to other factors beyond changes in memory -- for example, changes in how children represent the diagnostic visual features of each category [@long2018drawings]. 

To tease apart these alternatives, we investigated the development of children's ability to produce recognizable drawings of visual concepts when asked to draw from memory ("Can you draw a [rabbit]?) and when asked to draw a canonical exemplar from observation ("Can you draw this [rabbit] as it looks in the picture?") across two different geographical sites (San Jose, USA and Beijing, China).
To do so, we gathered digital drawings from a large sample of children 4-9 years of age in a controlled experimental setting.
While children in different communities may spend more or less time practicing drawing or use different visual conventions to draw [@cohn2012explaining; @willats2006making], most empirical studies on children's drawings have been conducted exclusively on children from the United States -- or focused on differences in educational practices between communities [@la2001children; @winner1989can; @huntsinger2011cultural].
To assess the degree to which visuomotor development accounts for any observed variation, we also measured each child's shape tracing abilities.

We had several predictions about the nature of the effects that we would observe. First, we predicted that only older children would be able to use the visual information present in the canonical photographs to improve their drawings, in keeping with accounts of naïve realism and contra a strong account of working memory limitations.
Second, we predicted that we would see convergence in the development of drawing abilities across both geographical sites, with older children becoming progressively better at producing recognizable drawings.
Finally, we predicted that most of the variance across geographical sites in drawing ability would be explainable by differences in visuomotor control, operationalized as performance on a shape tracing task; these primary analyses were pre-registered at https://osf.io/qymjr/.

Overall, we replicated prior work, showing strong and consistent developmental changes in the recognizability of children's drawings across development. 


# Methods


```{r plotting-params}
base_size_chosen=12; smooth_alpha=.2
```

```{r load-classifications}
classification_data <- read.csv(here::here('data/compiled/compiled_classifications/Classification_Outputs2760.csv')) %>%
  as_tibble() %>%
  mutate(age_numeric = age) %>%
  mutate(age = paste('age',age,sep="")) %>%
  mutate(age = as.factor(age)) %>%
  mutate(category = target_label) %>% 
  dplyr::select(-X, -X.1, -index) %>%
  mutate(site = case_when( is.na(str_locate(session_id,'photodraw_e2')[,1]) ~ "THU", 
                           !is.na(str_locate(session_id,'photodraw_e2')[,1]) ~ "CDM")) %>%
  mutate(site = as.factor(site)) 
```

```{r load-metadata}
# load and clean up "category" for tracing trials
all_meta <- read.csv(here::here('data/compiled/metadata/final_merged_metadata.csv')) %>%
    mutate(category = as.character(category)) %>%
    mutate(category = case_when(category == 'this square' ~ 'square',
                                category == 'this shape' ~ 'shape',                                
                                TRUE ~ as.character(category))) 

# subIDs are unique identifier in recognition data for THU, session_ids in CDM data
meta_thu <- all_meta %>%
  filter(site=='THU') %>%
  mutate(unique_ids = subID) %>%
  mutate(draw_duration = draw_duration / 1000) # in ms for thu, whoops

meta_cdm <- all_meta %>%
  filter(site=='CDM') %>%
  mutate(unique_ids = session_id) 

## unique IDs are now the identifier which will be used to join with machine/human recognition data
all_meta_cleaned <- meta_cdm %>%
  full_join(meta_thu) 
```

\
```{r load-tracing}
# import, standardized spliced session_ids, and join back together
# tracing IDs are read in by session_ids, so need these to join with metadata
tracing_thu <- read.csv(here::here('data/compiled/tracing_outputs/transformed_tracings.csv'))  %>%
  mutate(session_id =  paste0('Tsinghua_photodraw_',session_id)) %>%
  filter(site=='Tsinghua') %>%
  left_join(meta_thu, by=c('session_id','category')) %>%
  dplyr::select(-X.1, -X, -site.x, -filename.y) %>%
  rename(site = site.y, filename = filename.x) 

tracing_cdm <- read.csv(here::here('data/compiled/tracing_outputs/transformed_tracings.csv'))  %>%
  filter(site=='CDM') %>%
  mutate(session_id = paste0('photodraw_',session_id)) %>%
  left_join(meta_cdm, by=c('session_id','category')) %>%
  dplyr::select(-X.1, -X, -site.x, -filename.y) %>%
  rename(site = site.y, filename = filename.x) 
  
# for modeling tracing scores for each shape (square/shape) separately for each participant
all_tracing <- tracing_thu %>%
  full_join(tracing_cdm) 

# for per-subject tracing estimates
tracing_by_sub <- all_tracing %>%
  group_by(unique_ids) %>%
  summarize(avg_tracing_score = mean(rating))
```

```{r join-data}
model_classifications <- classification_data %>%
  mutate(unique_ids = session_id) %>%
  left_join(all_meta_cleaned %>% dplyr::select(unique_ids, category, condition, num_strokes, draw_duration, mean_intensity)) %>% # need to select columns so we don't get join errors 
  left_join(tracing_by_sub)
  

# bizarrely, there are a few drawings from THU for which stroke data didn't save -- technical error.
# this was checked directly in mongodb database -- no idea why, assuming bad internet.
tech_error_drawings <- model_classifications %>%
  filter(is.na(num_strokes))
# filter out tech error drawings from dataset (24)
model_classifications <- model_classifications%>%
  filter(!is.na(num_strokes)) 

```

```{r load-human-recognition}
humans <- read.csv(here::here('data/compiled/recognition_ratings/compiled_human_recognition.csv')) 
  

```

```{r join-human-rec-with-meta}
d <- humans %>%
  group_by(unique_ids, category, image_name_short, site, age) %>% # group by each drawing of each category
  summarize(prop_correct = mean(correct_or_not))  %>%
   left_join(all_meta_cleaned %>% dplyr::select(unique_ids, category, condition, num_strokes, draw_duration, mean_intensity)) %>%
  mutate(age_numeric = age) %>%
  mutate(age_numeric = as.double(age_numeric)) %>%
  mutate(age_numeric = case_when(age_numeric == 10 ~ 9, # merge 2 10-year-olds into THU data for 9-year-olds
                                 TRUE ~ age_numeric)) %>%
  left_join(tracing_by_sub) %>%
  mutate(site = fct_recode(site, "San Jose" = "CDM", "Beijing" = "THU"))%>%
  mutate(condition = fct_recode(condition, "From perception" = "P", "From memory" = "S")) 

humans_and_models_merged <- humans %>%
  group_by(unique_ids, category, image_name_short) %>% # group by each drawing of each category
  summarize(prop_correct = mean(correct_or_not)) %>%
  left_join(d, by=c('unique_ids','category')) %>% # join with classification data 
  filter(!is.na(age))
```

```{r count-participants}
participant_counts <- d %>%
  group_by(age_numeric, site) %>%
  summarize(num_participants = length(unique(unique_ids))) 

count_by_site <- d %>%
  group_by(site) %>%
  summarize(num_participants = length(unique(unique_ids))) 

count_by_category <- d %>%
  group_by(category) %>%
  summarize(num_drawings = length(unique(unique_ids)))

count_by_id <- d %>%
  group_by(unique_ids) %>%
  summarize(num_drawings = length(unique(category))) 
```

## Participants
265 participants were recruited from the San Jose Children’s Discovery museum, the Palo Alto Junior Museum and Zoo, and preschool and elementary schools outside of Beijing; approximately equal numbers of participants were recruited in Northern California and the Beijing area. We aimed to recruit approximately 120 children between 4-9 years of age after exclusions (i.e. 20 4-year-olds, 20 5-year-olds, etc.). In the US-based sample, 135 children participated; 6 participants were excluded, (3) for skipping more than 6 trials, and (3) for scribbling three or more times in a row; (6) participants were tested but their data was not recorded due to a technical error, and (2) participants never made it past the practice trials, leading to a final sample of 121 children. In the China based sample, `r count_by_site$num_participants[2]` children participated; an additional 8 participants were tested but their data was not recorded due to a technical error with the remote database. Two 10-year-olds (aged 10 years, 0 months and 10 years, 1 month) were accidentally tested and included in the 9-year-old age group. A complete breakdown of the number of subjects in each age group and site can be found in the Appendix, Table 1A.

Blank drawings were excluded from analysis, and drawings were randomly undersampled to have an equal number of drawings for each leave-one-out-classification. On average, each child contributed `r mean(count_by_id$num_drawings)` to analysis (min `r min(count_by_id$num_drawings)`, max = `r max(count_by_id$num_drawings)`).

## Task Procedure
Before beginning, a trained experimenter first told each child “After this game is over, someone is going to try to recognize what you were trying to draw. So, please draw so that someone else could try what you were trying to draw.” Children then completed a series of tracing trials designed to both familiarize them with the tablet interface and to obtain an estimate of their tracing abilities; children were first asked to trace a square and then a complex shape (see Figure X). Children used their fingers to draw and had a maximum of 30 seconds to produce their tracings/drawings on all trials; strokes could not be deleted once drawn. 

The primary experimental manipulation was the type of cue used to prompt children to produce a drawing. Specifically, there were two cue types: (1) a verbal cue provided by the experimenter in a 5s video clip (2) an photo cue which remained on screen continuously while children drew. In the verbal-cue condition, children heard “What about a [x]? Can you draw a [x]?” In the image-cue condition, children heard audio of the same experimenter said,  “What about this [x]? Can you draw the [x] as it looks in the picture?”. Children completed 12 drawing trials, with six categories each randomly assigned to each condition and displayed in a random order. For photo-cues trials, one of three possible exemplars was randomly chosen (see Appendix, Figure X for all stimuli). Children completed drawing trials were blocked by these two conditions; the condition order counterbalanced across subjects in each age group. All instructions were translated into Mandarin by a native speaker; see Appendix, Table X for exact translations.

## Measuring Tracing Accuracy
As in [@long2018drawings], we used an automated procedure for evaluating how accurately participants performed the tracing task, validated against empirical judgments of tracing quality. We decomposed tracing accuracy into two components: a shape error component and a spatial error component. Shape error reflects how closely the participant’s tracing matched the contours of the target shape; the spatial error reflects how closely the location, size, and orientation of the participant’s tracing matched the target shape.

To compute these error components, we applied an image registration algorithm, AirLab [@sandkuhler2018], to align each tracing to the target shape, yielding an affine transformation matrix that minimized the pixel-wise correlation distance between the aligned tracing, $T$, and the target shape, $S$: $Loss_{NCC} = - \frac{\sum S \cdot T - \sum E(S) E(T)}{N \sum Var(S) Var(T)}$, where $N$ is the number of pixels in both images.  The shape error was defined by the final correlation distance between the aligned tracing and the target shape. The spatial error was defined by the magnitude of three distinct error terms: location, orientation, and size error, derived by decomposing the affine transformation matrix above into translation, rotation, and scaling components, respectively. In sum, this procedure yielded four error values for each tracing: one value representing the shape error (i.e., the pixel-wise correlation distance) and three values representing the spatial error (i.e., magnitude of translation, rotation, scaling components). 

We used the same tracing quality ratings to obtained in [@long2018drawings] to assign weights to each of their error terms; adult observers ($N$=70) rated 1325 tracings (i.e., 50-80 tracings per shape per age) and evaluated “how well the tracing matches the target shape and is aligned to the position of the target shape” on a 5-point scale. An ordinal regression mixed-effects model to predict these 5-point ratings, which contained correlation distance, translation, rotation, scaling, and shape identity (square vs. star) as predictors, with random intercepts for rater.  This model yielded parameter estimates that could then be used to score each tracing in the dataset; we averaged scores for both shapes within session to yield a single tracing score for each participant.

## Measuring effort covariates
For each drawing trial, children had up to 30 seconds to complete their drawings with their fingers. We recorded both the final drawings and the parameters of each stroke produced by children, allowing us to estimate the amount of time children put into their drawings (e.g., end time of last stroke — start time of first stroke). As a second measure of effort, we also counted the number of strokes that children put into a given drawing. Finally, we estimated the proportion of the drawing canvas that was filled (e.g., 'ink used') by computing the proportion of each final drawing that were non-white pixels.

## Measuring drawing recognizability 

### Human recognition scores
We assessed the recognizability of each drawing via a recognition experiment. Adult participants were recruited via Prolific for a 15 minute experiment and compensated at $14/hour. Participants were shown a random subset of the drawings and asked "What does this look like?" and then selected an answer from the twelve categories that children were prompted to draw. Participants were encouraged to guess.   

### Automated recognition scores
We used a combination of neural neural activations and logistic regressions to obtain automated recognition scores, as per our pre-registered protocol. To encode the high-level visual features of each sketch, we used the VGG-19 architecture \cite{simonyan2014very}, a deep convolutional neural network pre-trained on Imagenet classification. We used model activations in the second-to-last layer of this network, which is the first fully connected layer of the network (FC6). Raw feature representations in this layer consist of flat 4096-dimensional vectors, to which we applied channel-wise normalization across all filtered drawings in the dataset.  Next, we used these features to train object category decoders. We then trained a 12-way logistic classifier with L2 regularization (tolerance = .1, regularization = .1), and used this classifier to estimate the category label for each drawing in the dataset.To avoid any bias due to imbalance in the distribution of drawings over categories, we randomly under sampled such that there were an equal number of drawings for each combination of geographical site (San Jose, Beijing) and the 12 categories. No additional metadata about the age of the child who produced each sketch was provided to the decoder. This procedure was repeated for each drawing in the dataset, yielding both a binary a recognition score and the softmax probability assigned to each target class in the dataset. 

## Statistical models
Two mixed effects models were fit to assess the factors that influenced the recognizability of the drawings that children produced. A first linear mixed effect model was fit to the recognizability scores assigned to each drawing, including fixed effects of children's age (in years), geographical site (San Jose vs. Beijing), and drawing task (photo-cue vs. label-cue), and the three way interaction between these key variables. We initially attempted to include random slopes for the effect of drawing task on each child (as this varied within-subjects), and random slopes for the effect the full three-way interactions between task, age, and site on each category. However, all models with this complicated random effects structure failed to converge, and the reported models the maximal random effects structure that did converge following @Barr2013. We then ran a second model predicting the human recognition scores for each drawing with the same fixed and random effect structure. 

In a secondary analysis, we aimed to understand the degree to which any of the above effects were mediated by (1) children's tracing abilities and (2) the amount of effort that children expended while drawings. We thus ran the same main model while also now including fixed effects of children's estimated tracing score (see above), the time children spent drawing (in seconds), the mean intensity of the drawing (i.e. percentage of non-white pixels), and the number of strokes children used. All predictors were scaled to have a mean of 0 and a standard deviation of 1. Finally, we also assessed the degree to which tracing ability development differed by geographical site, where tracing scores were modeled as a function of age (in years), site, and their interaction, with random intercepts for each participant. 
 

```{r}
full_model <- lmer(prop_correct ~ condition*scale(age_numeric)*site +
                        (1 | unique_ids) +
                        (condition | category),
      data = d)

full_model_out = summary(full_model)

```

```{r}
full_model_with_effort <- lmer(prop_correct ~ condition*scale(age_numeric)*site +
                      scale(avg_tracing_score) +
                      scale(mean_intensity) +
                      scale(draw_duration) +
                      scale(num_strokes) + 
                      (1 | unique_ids) +
                      (condition | category),
      data = d)

# frustratingly can't get anything with random slopes to converge!
full_model_with_effort_out = summary(full_model_with_effort)

```

 
# Results
Overall, we found relative consistency in the developmental trajectory of children's drawings. Figure \ref{fig:main_results} shows the recognizability of children's drawings at each age as a function of the drawing task and the geographical site that they were located in. We found no difference in the recognizability of children's drawings according to the task they were asked to do: drawings were equally recognizable when they were asked to "Draw this rabbit as it looks in the picture" and they were simply asked "What about a rabbit? Can you draw a rabbit." While we anticipated that older children might produce more recognizable drawings when drawing from observation -- as do some adults when presented with canonical photographs of object categories [@photodrawcogsci] -- this was not the case. All model coefficients can be found in Table 1. 

However, we did observe a main effect of geographical site: Children who were recruited outside of Beijing, China tended to produce more recognizable drawings than children who were recruited in San Jose, USA. In a second set of analyses, we aimed to understand the nature of this difference. We hypothesized that any differences in the recognizability of children's drawings across sites would be mediated by the amount of effort children expended while drawing or their tracing abilities. To test this hypothesis, we thus included children's average tracing scores and effort covariates measured for each drawing (average intensity, number of strokes used, and time spent drawing) as fixed effects into the same linear mixed-effects model. If these effort covariates explained the differences between sites that we observed, we reasoned that we should no longer observe a main effect of geographical location on drawing recognizability. Contra this hypothesis, however, we still observed a significant effect of geographical site (see Table 2), despite the fact that individual children's tracing abilities were strong predictors of how recognizable their drawings were.

```{r}
cor_by_age_by_site_by_cond <- d %>%  
  group_by(unique_ids,category,condition,age_numeric, site) %>%
  summarize(avg_cor = mean(prop_correct)) %>%
  group_by(age_numeric,condition, site) %>%
  multi_boot_standard(col = "avg_cor") 
```

```{r main-results, fig.env="figure", out.width = "\\textwidth", fig.pos = "H", fig.align = "center", fig.width=8, fig.height=4, fig.cap = "Proportion of drawings recognized as a function of the age of the child who completed each drawing, the geographical site they were tested at (San Jose vs. Beijing), and the type of drawing task they completed. Error bars show bootstrapped 95 percent cofnidence intervals."}
ggplot(cor_by_age_by_site_by_cond, aes(age_numeric,mean, col = condition)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position=position_dodge(width=.2)) +
  theme_few(base_size = base_size_chosen) + 
  labs(x='Age (in years)', y='Proportion recognized') +
  ylim(0,1) + 
  geom_smooth(span=20, alpha=.1) +
  scale_color_discrete(name="") +
  theme(legend.position = 'bottom') +
  geom_hline(yintercept = 1/12, linetype="dashed", color="grey") +
  facet_grid(~site)

```

```{r}
cleaned_names_full_model = c("Intercept","Task","Age","Site","Task*Age","Task*Site","Age*Site","Task*Age*Site")
rownames(full_model_out$coefficients)<-cleaned_names_full_model

# xtable(full_model_out$coefficients, digits=c(2,2,2,2,2,3),"Model coefficients from a  linear mixed mode predicting the recognizability of each drawing for the main experimental contrasts.")
```


```{r}
cleaned_names_full_model_with_effort = c("Intercept","Task","Age","Site","Est. tracing score","Avg. intensity", "Draw duration", "Number of strokes","Task*Age","Task&Site", "Age*Site","Task*Age*Site")

rownames(full_model_with_effort_out$coefficients)<-cleaned_names_full_model_with_effort

# xtable(full_model_with_effort_out$coefficients, digits=c(2,2,2,2,2,3),"Model coefficients from a linear mixed mode predicting the recognizability of each drawing as a function the both the main experimental contrasts (task, site, and age) as well as several effort covariates and individual's estimates tracing abilities.")
```


```{r get-descriptives-across-age}
draw_duration <- d %>%
  group_by(unique_ids,condition,age_numeric, site) %>%
  summarize(avg_draw_duration = mean(draw_duration)) %>%
  group_by(age_numeric,condition, site) %>%
  multi_boot_standard(col = "avg_draw_duration")

num_strokes <- d %>%
  group_by(unique_ids,condition,age_numeric,site) %>%
  summarize(avg_num_strokes = mean(num_strokes)) %>%
  group_by(age_numeric,condition, site) %>%
  multi_boot_standard(col = "avg_num_strokes") 

avg_intensity <- d %>%
  group_by(unique_ids,condition,age_numeric,site) %>%
  summarize(avg_intensity = mean(mean_intensity)) %>%
  group_by(age_numeric,condition, site) %>%
  multi_boot_standard(col = "avg_intensity")
```


```{r plot-effort}
## Make compiled plot of descriptives
base_size_chosen=14 # size of text in plots
smooth_alpha=.2

p1=ggplot(draw_duration, aes(age_numeric,mean, color=condition)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = 0.5)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Draw duration (s)') +
  theme(legend.position = "none") + 
  ylim(0,30) +
  facet_grid(~site)

p2=ggplot(avg_intensity, aes(age_numeric,mean, color=condition)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = 0.5)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Ink used (mean intensity)') +
  theme(legend.position = "none") + 
  ylim(.02,.08) +
  facet_grid(~site)

p3=ggplot(num_strokes, aes(age_numeric,mean, color=condition)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = 0.5)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Number of strokes') +
  theme(legend.position = "none") +
  ylim(0,15) +
  facet_grid(~site)
```

```{r effort-covariates, fig.env="figure", out.width = "\\textwidth", fig.pos = "H", fig.align = "center", fig.width=8, fig.height=4, fig.cap = "Effort covariates measured during the drawing task -- amount of time spent drawing, amount of 'ink' used, and number of strokes used -- as function of the age of the child who completed each drawing, the geographical site they were tested at (San Jose vs. Beijing), and the type of drawing task they completed. Error bars show bootstrapped 95 percent cofnidence intervals."}
cowplot::plot_grid(p1,p2,p3,nrow=1)
```

```{r}
tracing_scores_indiv <- d %>%
  ungroup() %>%
  distinct(unique_ids, avg_tracing_score, site, age_numeric)

tracing_scores <- tracing_scores_indiv %>%
  group_by(age_numeric, site) %>%
  multi_boot_standard(col = 'avg_tracing_score')
```

```{r tracing, fig.env="figure", out.width = "\\textwidth", fig.pos = "H", fig.align = "center", fig.width=8, fig.height=4, fig.cap = "Average tracing scores across age and site; each dot represents an average tracing score obtained for each participant and are slightly jittered. Error bars represent bootstrapped 95 percent confidence intervals."} 
ggplot(tracing_scores, aes(age_numeric,mean)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  geom_jitter(data = tracing_scores_indiv, aes(x=age_numeric, y=avg_tracing_score), width=.2, height=.05, alpha=.5) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Tracing score') +
  theme(legend.position = "none") +
  geom_smooth(col='grey', span = 10,alpha=smooth_alpha)  +
  facet_grid(~site) +
  ylim(0,4)
```


# Discussion



# Acknowledgements  
We thank Yi Feng and Megan Merrick for assistance with data collection, with whom this project would have not been possible. We also gratefully acknowledge the San Jose Children's Discovery Museum for their collaboration. We are also thankful to all of the parents at XX and XX preschool whose children participated. We also thank the members of the Stanford Language and Cognition lab for their feedback. This work was funded by an NSF SPRF-FR Grant \#1714726 to BLL and a Jacobs Foundation Fellowship to MCF. 

\newpage

# References
```{r create_r-references}
r_refs(file = "references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
