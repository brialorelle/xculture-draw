---
title             : "Consistency and variability in the development of children's drawings of object categories"
shorttitle        : "Development of drawing"

author: 
  - name          : "Bria Long"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Jane Stanford Way, Stanford CA 94305"
    email         : "bria@stanford.edu"
  - name          : "Ying Wang"
    affiliation   : "2"
  - name          : "Stella Christie"
    affiliation   : "2"
  - name          : "Michael C. Frank"
    affiliation   : "1"
  - name          : "Judith E. Fan"
    affiliation   : "3"

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "Tsinghua University"
  - id            : "3"
    institution   : "University of California, San Diego"

authornote: |

abstract: |
 Childen's drawings of common object categories become dramatically more recognziable across development. What are the major factors that explain this developmental change? Here, we examined the degree to which these developmental changes in recognizibility vary across different drawing tasks (i.e. drawing from observation vs. from memory), geographical locations (San Jose, US vs. Beijing, China),and with children's tracing abilities. To do so, we collecting digital drawings of object categories (e.g., cat, airplane) from 4-9 year-olds (N=253). Overall, we show broad consistency in these developmental trajectories acros both drawing tasks and these two geographical locations, and we find that children's tracings abilites are good predictors of the recognizability of the drawings that they produce. In addition, we find that children recruited near Beijing, China producedmore recognizable drawings but showed similar tracing abilities to childen recruited near San Jose, CA. Overall, this work suggests that the developmental trajectory of children's drawings are remarkably consistent and not easily explainable by changes in their visuomotor control. 
 
keywords          : "children's drawings, visual production, tracing, object recognition, visuomotor control"
wordcount         : "X"

bibliography      : ["references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(tidyverse)
library(here)
library(assertthat)
library(langcog)
library(ggthemes)
library(knitr)
library(lmerTest)
library(lme4)
library(xtable)
library(viridis)
```

As humans, we have many powerful tools to externalize what we know, including language and gesture. 
One tool that has been transformative for human cognition and culture is graphical representation, which allows people to encode their thoughts in a visible, durable format. 
Drawing is an important case study in graphical representation, being a technique that dates back 60,000 years [@hoffmann2018u], well before the emergence of symbolic writing systems, and is practiced in many cultures.

In modern times, drawings are produced prolifically by children from an early age. 
Figurative drawings have long provided inspiration for scientists investigating children's emerging cognitive abilities [@minsky1972artificial], and accordingly a long history of work has examined changes in children's drawings across development [@piaget1929child; @kellogg1969analyzing; @karmiloff1990constraints; @fury1997children].
Indeed, there appear to be dramatic changes in how children encode diagnostic visual information in their drawings across age; younger children (4-5 years) tend to include fewer cues in their drawings to differentiate between target concepts (e.g., \textit{adult} vs. \textit{child}) than older children, who enrich their drawings with more diagnostic part [@sitton1992drawing] and relational [@light1983effects] information. 
Overall, younger children produce drawings of object categories that are less diagnostic of the categories they are trying to depict [@long2018drawings; @long2021parallel]. 
 
What drives these dramatic changes in children's drawings across development? 
A common view is that these changes are driven primarily by children's increasing ability to plan and control their motor movements [@freeman1987current; @rehrig2018does]. 
While such changes in visuomotor control are clearly important, this view fails to account for other important constraints, such as how well children are able to access previously acquired semantic knowledge about each target concept and maintain this information in working memory during drawing production.
Thus, drawing is a skill that likely also relies on children's evolving perceptual category representations [@long2018drawings; @natu2016development; @dekker2011dorsal] as well as their increasing working memory capacity [@pailian2016visual].

One idea is that a principal reason younger children produce less recognizable drawings is because they simply have more difficulty recalling the relevant perceptual features of different categories: that is, when asked to "draw a [rabbit]", they may struggle to conjure up the relevant perceptual details and then hold in mind what rabbits tend to look like. 
On this account, providing children with additional perceptual information about different categories -- for example, canonical photographs of typical exemplars -- could help them improve their drawings of these categories, as it does with adults [@fan2020relating]. 
However, prior work also suggests that younger children tend to draw what they know about objects rather than integrate the information in their immediate perceptual experience.  
For example, when asked to draw from observation, younger children tend to include features that are not visible from their vantage point, yet are diagnostic of category membership (e.g., a handle on a \textit{mug}) [@bremmer1984prior; @barrett1976symbolism], and only omit these features later in development. 
Similarly, young children will insist that their nearly identical drawings of different concepts (e.g., balloon and person) unambiguously refer to different things [@bloom1998intention]. 
Thus, an alternative possibility is that only older children may be able to produce more recognizable drawings when provided with canonical exemplars of different categories. 
On this account, changes in children's drawings from memory across age may be largely due to other factors beyond changes in memory -- for example, changes in how children represent the diagnostic visual features of each category [@long2018drawings]. 

To tease apart these alternatives, we investigated the development of children's ability to produce recognizable drawings of visual concepts when asked to draw from memory ("Can you draw a [rabbit]?) and when asked to draw a canonical exemplar from observation ("Can you draw this [rabbit] as it looks in the picture?") across two different geographical sites (San Jose, USA and Beijing, China).
To do so, we gathered digital drawings from a large sample of children 4-9 years of age in a controlled experimental setting.
While children in different communities may spend more or less time practicing drawing or use different visual conventions to draw [@cohn2012explaining; @willats2006making], most empirical studies on children's drawings have been conducted exclusively on children from the United States -- or focused on differences in educational practices between communities [@la2001children; @winner1989can; @huntsinger2011cultural].
In addition, we assessed the degree to which visuomotor development accounts for developmental changes in drawing recognizability by measuring each child's shape tracing abilities.

We had several predictions about the nature of the effects that we would observe. First, we predicted that only older children would be able to use the visual information present in the canonical photographs to improve their drawings, in keeping with accounts of naïve realism and contra a strong account of working memory limitations.
Second, we predicted that we would see convergence in the development of drawing abilities across both geographical sites, with older children becoming progressively better at producing recognizable drawings.
Finally, we predicted that most of the variance across geographical sites in drawing ability would be explainable by differences in visuomotor control, operationalized as performance on a shape tracing task; these primary analyses were pre-registered at https://osf.io/qymjr/.

<!-- Overall, we replicated prior work [@long2021parallel], showing strong and consistent developmental changes in the recognizability of children's drawings across development across both geographical sites. In addition, children of all ages produced equally recognizable drawings across both drawing tasks, suggesting that working memory limitations are not amoung the primary factors that influence the recognizability of children's drawings. Finally, we found that children recruited  -->


# Methods
```{r plotting-params}
base_size_chosen=12; smooth_alpha=.2
```

```{r load-classifications}
classification_data <- read.csv(here::here('data/compiled/compiled_classifications/Classification_Outputs2760.csv')) %>%
  as_tibble() %>%
  mutate(age_numeric = age) %>%
  mutate(age = paste('age',age,sep="")) %>%
  mutate(age = as.factor(age)) %>%
  mutate(category = target_label) %>% 
  dplyr::select(-X, -X.1, -index) %>%
  mutate(site = case_when( is.na(str_locate(session_id,'photodraw_e2')[,1]) ~ "THU", 
                           !is.na(str_locate(session_id,'photodraw_e2')[,1]) ~ "CDM")) %>%
  mutate(site = as.factor(site)) 
```

```{r load-metadata}
# load and clean up "category" for tracing trials
all_meta <- read.csv(here::here('data/compiled/metadata/final_merged_metadata.csv')) %>%
    mutate(category = as.character(category)) %>%
    mutate(category = case_when(category == 'this square' ~ 'square',
                                category == 'this shape' ~ 'shape',                                
                                TRUE ~ as.character(category))) 

# subIDs are unique identifier in recognition data for THU, session_ids in CDM data
meta_thu <- all_meta %>%
  filter(site=='THU') %>%
  mutate(unique_ids = subID) %>%
  mutate(draw_duration = draw_duration / 1000) # in ms for thu, whoops

meta_cdm <- all_meta %>%
  filter(site=='CDM') %>%
  mutate(unique_ids = session_id) 

## unique IDs are now the identifier which will be used to join with machine/human recognition data
all_meta_cleaned <- meta_cdm %>%
  full_join(meta_thu) 
```

\
```{r load-tracing}
# import, standardized spliced session_ids, and join back together
# tracing IDs are read in by session_ids, so need these to join with metadata
tracing_thu <- read.csv(here::here('data/compiled/tracing_outputs/transformed_tracings.csv'))  %>%
  mutate(session_id =  paste0('Tsinghua_photodraw_',session_id)) %>%
  filter(site=='Tsinghua') %>%
  left_join(meta_thu, by=c('session_id','category')) %>%
  dplyr::select(-X.1, -X, -site.x, -filename.y) %>%
  rename(site = site.y, filename = filename.x) 

tracing_cdm <- read.csv(here::here('data/compiled/tracing_outputs/transformed_tracings.csv'))  %>%
  filter(site=='CDM') %>%
  mutate(session_id = paste0('photodraw_',session_id)) %>%
  left_join(meta_cdm, by=c('session_id','category')) %>%
  dplyr::select(-X.1, -X, -site.x, -filename.y) %>%
  rename(site = site.y, filename = filename.x) 
  
# for modeling tracing scores for each shape (square/shape) separately for each participant
all_tracing <- tracing_thu %>%
  full_join(tracing_cdm) 

# for per-subject tracing estimates
tracing_by_sub <- all_tracing %>%
  group_by(unique_ids) %>%
  summarize(avg_tracing_score = mean(rating))
```

```{r join-data}
model_classifications <- classification_data %>%
  mutate(unique_ids = session_id) %>%
  left_join(all_meta_cleaned %>% dplyr::select(unique_ids, category, condition, num_strokes, draw_duration, mean_intensity)) %>% # need to select columns so we don't get join errors 
  left_join(tracing_by_sub)
  

# bizarrely, there are a few drawings from THU for which stroke data didn't save -- technical error.
# this was checked directly in mongodb database -- no idea why, assuming bad internet.
tech_error_drawings <- model_classifications %>%
  filter(is.na(num_strokes))
# filter out tech error drawings from dataset (24)
model_classifications <- model_classifications%>%
  filter(!is.na(num_strokes)) 

```

```{r load-human-recognition}
humans <- read.csv(here::here('data/compiled/recognition_ratings/compiled_human_recognition.csv')) 
  
```

```{r join-human-rec-with-meta}
d <- humans %>%
  group_by(unique_ids, category, image_name_short, site, age) %>% # group by each drawing of each category
  summarize(prop_correct = mean(correct_or_not))  %>%
   left_join(all_meta_cleaned %>% dplyr::select(unique_ids, category, condition, num_strokes, draw_duration, mean_intensity)) %>%
  mutate(age_numeric = age) %>%
  mutate(age_numeric = as.double(age_numeric)) %>%
  mutate(age_numeric = case_when(age_numeric == 10 ~ 9, # merge 2 10-year-olds into THU data for 9-year-olds
                                 TRUE ~ age_numeric)) %>%
  left_join(tracing_by_sub) %>%
  mutate(site = fct_recode(site, "San Jose" = "CDM", "Beijing" = "THU"))%>%
  mutate(condition = fct_recode(condition, "From perception" = "P", "From memory" = "S")) 

humans_and_models_merged <- humans %>%
  group_by(unique_ids, category, image_name_short) %>% # group by each drawing of each category
  summarize(prop_correct = mean(correct_or_not)) %>%
  left_join(model_classifications, by=c('unique_ids','category')) %>% # join with classification data 
  filter(!is.na(age))
```

```{r count-participants}
participant_counts <- d %>%
  group_by(age_numeric, site) %>%
  summarize(num_participants = length(unique(unique_ids))) 

count_by_site <- d %>%
  group_by(site) %>%
  summarize(num_participants = length(unique(unique_ids))) 

count_by_category <- d %>%
  group_by(category) %>%
  summarize(num_drawings = length(unique(unique_ids)))

count_by_id <- d %>%
  group_by(unique_ids) %>%
  summarize(num_drawings = length(unique(category))) 
```

## Participants
265 participants were recruited from the San Jose Children’s Discovery museum, the Palo Alto Junior Museum and Zoo, and preschool and elementary schools outside of Beijing; approximately equal numbers of participants were recruited in Northern California and the Beijing area. Our goal was to recruit 120 children between 4-9 years of age after exclusions (i.e. 20 4-year-olds, 20 5-year-olds, etc.) at each geographical site. In the San Jose based sample, 135 children participated in the experiment; 6 participants were excluded, (3) for skipping more than 6 drawing trials and (3) for scribbling three or more times in a row. Six additional participants were tested but their data was not recorded due to a technical error, and two participants never advanced past the practice trials, leading to a final sample of `r count_by_site$num_participants[1]` children. In the Beijing based sample, `r count_by_site$num_participants[2]` children participated; an additional 8 participants were tested but their data was not recorded due to a technical error with the remote database. Two 10-year-olds (aged 10 years, 0 months and 10 years, 1 month) were accidentally tested and included in the 9-year-old age group. On average, each child contributed `r mean(count_by_id$num_drawings)` to analysis (min `r min(count_by_id$num_drawings)`, max = `r max(count_by_id$num_drawings)`). No additional demographic data was recorded about the participants. This protocol was approved by both the Institutional Review Board at [blinded] (43992, Development of Children's Drawing Abilities) and the Department of Psychology Ethics Committee at [blinded] in Beijing, China.

## Task Procedure
Before beginning, a trained experimenter first told each child “After this game is over, someone is going to try to recognize what you were trying to draw. So, please draw so that someone else could try what you were trying to draw.” A native English speaker gave these instructions to the San Jose based sample, and a native Mandarin speaker gave a translation of these instructions to the Beijing based sample. The rest of the instructions were given via audio and video cues of native English/Mandarin speakers, respectively; all experimental code, videos, and stimuli are available on the public repository for this project. All children used their fingers to draw on an Ipad Pro (12.9") that was set at a slight angle on a table via a tablet case.  All experimental code, videos, and data are available on the public repository for this project.

Children first completed two series of tracing trials to obtain an estimate of their tracing abilities; children were first asked to trace a square and then a complex shape (see Figure \ref{fig:example-tasks}). Children had maximum of 30 seconds to produce their tracings/drawings on all trials, and strokes could not be deleted once drawn; stroke-by-stroke data for each drawing was stored on a remote server.

The primary experimental manipulation was the drawing task that children participated in; each child completed both tasks and the order of tasks was counterbalanced across participants. To elicit drawings from memory, children saw a short video clip where an experimenter said,  “What about a [cat]? Can you draw a [cat]?” (exact Mandarin translation has trouble in latex). To elicit drawings from observation, children heard audio of the same experimenter who said,  “What about this [cat]? Can you draw the [cat] as it looks in the picture?”  (Mandarin translation) while seeing a canonical photograph of each category; this photograph then remained on the screen for the duration of the drawing trial.  This photograph was randomly sampled from one of three possible exemplars. Overall, each child completed 2 tracing trials and 12 drawing trials, with six categories randomly assigned to each condition and displayed in a random order. Task examples are shown in Figure \ref{fig:example-tasks}.

```{r example-tasks, fig.env = "figure", out.width="100%", fig.pos = "H", fig.cap = "Example trials from the tracing assessment and two drawing tasks."}
knitr::include_graphics("figs/task_examples.png")
```

## Measuring effort covariates
For each drawing trial, children had up to 30 seconds to complete their drawings with their fingers. We recorded both the final drawings and the parameters of each stroke produced by children, allowing us to estimate the amount of time children put into their drawings (e.g., end time of last stroke — start time of first stroke). As a second measure of effort, we also counted the number of strokes that children put into a given drawing. Finally, we estimated the proportion of the drawing canvas that was filled (e.g., 'ink used') by computing the proportion of each final drawing that were non-white pixels.

## Measuring Tracing Accuracy
As in [@long2018drawings], we used an automated procedure for evaluating how accurately participants performed the tracing task, validated against empirical judgments of tracing quality. We decomposed tracing accuracy into two components: a shape error component and a spatial error component. Shape error reflects how closely the participant’s tracing matched the contours of the target shape; the spatial error reflects how closely the location, size, and orientation of the participant’s tracing matched the target shape.

To compute these error components, we applied an image registration algorithm, AirLab [@sandkuhler2018], to align each tracing to the target shape, yielding an affine transformation matrix that minimized the pixel-wise correlation distance between the aligned tracing, $T$, and the target shape, $S$: $Loss_{NCC} = - \frac{\sum S \cdot T - \sum E(S) E(T)}{N \sum Var(S) Var(T)}$, where $N$ is the number of pixels in both images.  The shape error was defined by the final correlation distance between the aligned tracing and the target shape. The spatial error was defined by the magnitude of three distinct error terms: location, orientation, and size error, derived by decomposing the affine transformation matrix above into translation, rotation, and scaling components, respectively. In sum, this procedure yielded four error values for each tracing: one value representing the shape error (i.e., the pixel-wise correlation distance) and three values representing the spatial error (i.e., magnitude of translation, rotation, scaling components). 

We used the same tracing quality ratings to obtained in [@long2018drawings] to assign weights to each of their error terms; adult observers ($N$=70) rated 1325 tracings (i.e., 50-80 tracings per shape per age) and evaluated “how well the tracing matches the target shape and is aligned to the position of the target shape” on a 5-point scale. An ordinal regression mixed-effects model to predict these 5-point ratings, which contained correlation distance, translation, rotation, scaling, and shape identity (square vs. star) as predictors, with random intercepts for rater.  This model yielded parameter estimates that could then be used to score each tracing in the dataset; we averaged scores for both shapes within session to yield a single tracing score for each participant.


## Measuring drawing recognizability 

### Human recognition scores
We assessed the recognizability of each drawing via an online recognition experiment. Adult participants based in the U.S. were recruited via Prolific for a 15 minute experiment,compensated at $14/hour, and recognized a random subset of around 140 drawings that were balanced with respect to age, category, and site. Each drawing was recognized by 10 participants. Participants were shown a were shown these drawings in a random order and asked "What does this look like?". Participants selected an answer from the twelve categories that children were prompted to draw and were were encouraged to guess. No participants were excluded from analysis for missing a catch trial (a free response describing the task that were completing). These binary recognition scores were then averaged across participants to yield a recognition score (e.g., recognized by 80% of participants) for each drawing.

### Automated recognition scores
We used a combination of deep CNN activations and logistic regressions to obtain automated recognition scores, as per our pre-registered protocol. To encode the high-level visual features of each sketch, we used the VGG-19 architecture \cite{simonyan2014very}, a deep convolutional neural network pre-trained on Imagenet classification. We used model activations in the second-to-last layer of this network, which is the first fully connected layer of the network (FC6). Raw feature representations in this layer consist of flat 4096-dimensional vectors, to which we applied channel-wise normalization across all filtered drawings in the dataset.  Next, we used these features to train object category decoders. We then trained a 12-way logistic classifier with L2 regularization (tolerance = .1, regularization = .1), and used this classifier to estimate the category label for each drawing in the dataset.To avoid any bias due to imbalance in the distribution of drawings over categories, we randomly under sampled such that there were an equal number of drawings for each combination of geographical site (San Jose, Beijing) and the 12 categories. No additional metadata about the age of the child who produced each sketch was provided to the decoder. This procedure was repeated for each drawing in the dataset, yielding a binary a recognition score for each drawing.

## Statistical models
```{r}
human_v_model = cor.test(humans_and_models_merged$prop_correct, humans_and_models_merged$correct_or_not)
```

To assess our main hypotheses, we fit linear mixed effects models to the human recogniton scores to assess the factors that influenced the recognizability of the drawings that children produced. A first linear mixed effect model was fit to the recognizability scores assigned to each drawing, including fixed effects of children's age (in years), geographical site (San Jose vs. Beijing), and drawing task (drawing from perception vs. memory) and the three way interaction between these key variables. We initially planned to include random slopes for the effect of drawing task on each child (as this varied within-subjects), and random slopes for the effect the full three-way interactions between task, age, and site on each category. However, all models with this complicated random effects structure failed to converge, and the reported models the maximal random effects structure that did converge.

In a secondary analysis, we aimed to understand the degree to which any of the above effects were mediated by (1) children's tracing abilities and (2) the amount of effort that children expended while drawings. We thus ran the same main model while also now including fixed effects of children's estimated tracing score (see above), the time children spent drawing (in seconds), the mean intensity of the drawing (i.e. percentage of non-white pixels), and the number of strokes children used. All predictors were scaled to have a mean of 0 and a standard deviation of 1. Finally, we also assessed the degree to which tracing ability development differed by geographical site, where tracing scores were modeled as a function of age (in years), site, and their interaction, with random intercepts for each participant. 
 
This anlaysis plan was pre-registered at https://osf.io/qymjr/; however, we had initially planned to use automated recognition scores as our main dependent variable, following prior work [@long2021parallel]; however, we found that these automated recognition scores were only modestly correlated with human recognition scores ($r$ = `r round(human_v_model$estimate,2)`, $t$ = `r round(human_v_model$estimate,2)`, p < .001); furthermore, descriptive plots of the automated recognition scores revealed a difference between the two drawing tasks that was not evident in the human data. To be conservative, we thus fit all of our models to the human recognition data but have made all model classifications available for future researcs. 

 
```{r}
full_model <- lmer(prop_correct ~ condition*scale(age_numeric)*site +
                        (1 | unique_ids) +
                        (condition | category),
      data = d)

full_model_out = summary(full_model)

```

```{r}
full_model_with_effort <- lmer(prop_correct ~ condition*scale(age_numeric)*site +
                      scale(avg_tracing_score) +
                      scale(mean_intensity) +
                      scale(draw_duration) +
                      scale(num_strokes) + 
                      (1 | unique_ids) +
                      (condition | category),
      data = d)

# frustratingly can't get anything with random slopes to converge!
full_model_with_effort_out = summary(full_model_with_effort)

```

 
# Results
Overall, we found relative consistency in the developmental trajectory of children's drawings. Figure \ref{fig:main-results} shows the recognizability of children's drawings at each age as a function of the drawing task and the geographical site that they were located in; All model coefficients can be found in Table 1.  First, we found consistent and steady changes in the recognizability of children's drawings across this age range, replicating prior work with observational datasets collected at a free-standing kiosk [@long2021parallel]. Critically, these changes in recognizability did not depend on the drawing task children completed: children's drawings were equally recognizable when they were asked to "Draw this rabbit as it looks in the picture" and they were simply asked "What about a rabbit? Can you draw a rabbit." While we anticipated that older children might produce more recognizable drawings when drawing from observation -- as do adults when presented with canonical photographs of object categories [@yang2021visual] -- this was not the case. 

However, we did observe a main effect of geographical site: Children who were recruited near Beijing, China tended to produce more recognizable drawings than children who were recruited near San Jose, USA. In a second set of analyses, we aimed to understand the nature of this difference in recognizability. We hypothesized that any differences in the recognizability of children's drawings across sites would be mediated by the amount of effort children expended while drawing or their tracing abilities.  We thus first examined how effort and tracing abilities varied across task, age, and geographical location. Figure \ref{fig:effort-covariates} shows three effort covariates - average intensity, number of strokes used, and time spent drawing -- measured for each drawing as a function of children's age, drawing task, and geographical site: effort covariates measured for each drawing; children tended to spend more time drawing when asked to draw "the [rabbit] as it looks in the picture", and children recruited outside Beijing tended to spend more time drawing overall. In contrast, children's tracing abilities did not vary across geographical site: Figure \ref{fig:tracing} shows the average tracing score at each age at each site. While children's tracing abilities increased steadily with age, as in prior work [@long2021parallel], children recruited at both sites produced equally good shape tracings.

Next, we included children's average tracing scores and effort covariates measured for each drawing (average intensity, number of strokes used, and time spent drawing) as fixed effects into a second linear mixed-effects model with the same fixed and random effects structure as used above. If these effort covariates accounted the differences between sites that we observed, we reasoned that we should no longer observe a main effect of geographical location on drawing recognizability. Contra this hypothesis, however, we still observed a significant effect of geographical site (see Table 2), despite the fact that individual children's tracing abilities were strong predictors of how recognizable their drawings were. In addition, the amount of time a child spent drawing  was a significant negative predictor of recognizability -- indicating that the amount of effort that children expended did not necessarily result in a drawing that was more recognizable to others. 

Thus, these results suggest that these two groups of children differ in their ability to produce recognizable drawings of object categories in a way that is not esaily explainable by effort covariates or tracing abilities, contra previous accounts. In a third set of analyses, we examined how the developmental trajectory of the recognizability of children's drawings may differ for different object categories. Intuitively, some object categories (e.g., cat) may be easier to draw than others (e.g., watch), resulting in shallower or steeper changes in recognizability over age. However, we had no strong hypotheses about how these trajectories might additionally vary with geographical locations. 

Figure \ref{fig:item-effects} shows the recognizability of children's drawings for each of the 12 categories a function of geographical location and children's age and reveal considerable variability.  For example, certain categories (e.g., cats and rabbits) were overall more difficult for children to depict such that they were distinct from one another, particularly for younger children. In addition,  children recruited near Beijing produced more recognizable drawings of certain categories at all ages -- including airplanes, birds, and rabbits -- while older children recruited near San Jose produced more recognizable drawings of bikes.  Figure \ref{fig:example-drawings} shows recognizable, randomly sampled example drawings from 6-year-olds at each category, site, and condition.

```{r render-drawings, include=FALSE}
# set.seed(123)
# set.seed(234)
# sample_drawings <- d %>%
#   filter(prop_correct>.8) %>%
#   filter(age_numeric==6) %>%
#   group_by(category, condition, site) %>%
#   sample_n(1)
#   
# sample_drawings <- sample_drawings %>%
#   mutate(image_path = here::here('data/compiled/drawings/object_drawings', image_name_short)) %>%
#   mutate(new_image_path = here::here('data/compiled/drawings/sample_drawings2', image_name_short)) 
# 
# dir.create(here::here('data/compiled/drawings/sample_drawings2'))
# 
# file.copy(sample_drawings$image_path, sample_drawings$new_image_path)

```

```{r check-drawing-time}
# drawing_time <- lmer(draw_duration ~ condition*scale(age_numeric)*site +
#                       (1 | unique_ids) +
#                       (1 | category),
#       data = d)
# 
# # frustratingly can't get anything with random slopes to converge!
# drawing_time_out = summary(drawing_time)

```

```{r compile-main-results}
cor_by_age_by_site_by_cond <- d %>%  
  group_by(unique_ids,category,condition,age_numeric, site) %>%
  summarize(avg_cor = mean(prop_correct)) %>%
  group_by(age_numeric,condition, site) %>%
  multi_boot_standard(col = "avg_cor") 

cor_by_age_by_site_by_cond_indiv <- d %>%  
  group_by(unique_ids,category,condition,age_numeric, site) %>%
  summarize(avg_cor = mean(prop_correct))
```

```{r main-results, fig.env="figure", out.width = "\\textwidth", fig.pos = "H", fig.align = "center", fig.width=8, fig.height=5, fig.cap = "Proportion of drawings recognized as a function of the age of the child who completed each drawing, the geographical site they were tested at (San Jose vs. Beijing), and the type of drawing task they completed. Individual data points represent drawings within each condition by an individual participant and are slightly jittered. Error bars show bootstrapped 95 percent cofnidence intervals."}
ggplot(cor_by_age_by_site_by_cond, aes(age_numeric,mean, col = condition)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position=position_dodge(width=.2)) +
  geom_point(data = cor_by_age_by_site_by_cond_indiv, aes(x=age_numeric, y=avg_cor), position=position_jitterdodge(jitter.width = NULL,
  jitter.height = .001,
  dodge.width = 0.2,
  seed = 123), alpha=.2) +
  theme_few(base_size = base_size_chosen) + 
  labs(x='Age (in years)', y='Proportion drawings recognized') +
  ylim(-.01,1.1) + 
  geom_smooth(span=20, alpha=.1) +
  scale_color_discrete(name="") +
  theme(legend.position = 'bottom') +
  geom_hline(yintercept = 1/12, linetype="dashed", color="grey") +
  facet_grid(~site)

```

```{r output-models-1}
cleaned_names_full_model = c("Intercept","Task","Age","Site","Task*Age","Task*Site","Age*Site","Task*Age*Site")
rownames(full_model_out$coefficients)<-cleaned_names_full_model

# xtable(full_model_out$coefficients, digits=c(2,2,2,2,2,3),"Model coefficients from a  linear mixed mode predicting the recognizability of each drawing for the main experimental contrasts.")
```

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & Estimate & Std. Error & df & t value & Pr($>$$|$t$|$) \\
  \hline
Intercept & 0.73 & 0.02 & 31.62 & 34.74 & 0.000 \\
  Task & -0.02 & 0.02 & 17.29 & -0.87 & 0.397 \\
  Age & 0.17 & 0.02 & 373.04 & 11.21 & 0.000 \\
  Site & 0.05 & 0.02 & 385.12 & 2.39 & 0.017 \\
  Task*Age & -0.02 & 0.01 & 2622.52 & -1.47 & 0.141 \\
  Task*Site & -0.00 & 0.02 & 2631.48 & -0.20 & 0.842 \\
  Age*Site & -0.00 & 0.02 & 388.66 & -0.06 & 0.951 \\
  Task*Age*Site & 0.00 & 0.02 & 2639.46 & 0.21 & 0.834 \\
   \hline
\end{tabular}
\caption{Table 1. Model coefficients from a linear mixed mode predicting the recognizability of each drawing for the main experimental contrasts.}
\end{table}

```{r output-models-2}
cleaned_names_full_model_with_effort = c("Intercept","Task","Age","Site","Est. tracing score","Avg. intensity", "Draw duration", "Number of strokes","Task*Age","Task&Site", "Age*Site","Task*Age*Site")

rownames(full_model_with_effort_out$coefficients)<-cleaned_names_full_model_with_effort

# xtable(full_model_with_effort_out$coefficients, digits=c(2,2,2,2,2,3),"Model coefficients from a linear mixed mode predicting the recognizability of each drawing as a function the both the main experimental contrasts (task, site, and age) as well as several effort covariates and individual's estimates tracing abilities.")
```


\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & Estimate & Std. Error & df & t value & Pr($>$$|$t$|$) \\
  \hline
Intercept & 0.72 & 0.02 & 34.02 & 37.49 & 0.000 \\
  Task & -0.02 & 0.02 & 17.55 & -1.19 & 0.249 \\
  Age & 0.14 & 0.02 & 376.68 & 9.30 & 0.000 \\
  Site & 0.07 & 0.02 & 459.46 & 3.58 & 0.000 \\
  Est. tracing score & 0.06 & 0.01 & 249.21 & 5.71 & 0.000 \\
  Avg. intensity & 0.01 & 0.01 & 2839.40 & 1.90 & 0.058 \\
  Draw duration & -0.03 & 0.01 & 1692.69 & -3.42 & 0.001 \\
  Number of strokes & 0.03 & 0.01 & 2655.21 & 4.67 & 0.000 \\
  Task*Age & -0.02 & 0.01 & 2623.63 & -1.72 & 0.086 \\
  Task\&Site & -0.01 & 0.02 & 2630.59 & -0.45 & 0.655 \\
  Age*Site & -0.01 & 0.02 & 415.59 & -0.74 & 0.462 \\
  Task*Age*Site & 0.00 & 0.02 & 2638.72 & 0.18 & 0.858 \\
   \hline
\end{tabular}
\caption{Table 2. Model coefficients from a linear mixed mode predicting the recognizability of each drawing as a function the both the main experimental contrasts (task, site, and age) as well as several effort covariates and individual's estimates tracing abilities.}
\end{table}


```{r get-descriptives-across-age}
draw_duration <- d %>%
  group_by(unique_ids,condition,age_numeric, site) %>%
  summarize(avg_draw_duration = mean(draw_duration)) %>%
  group_by(age_numeric,condition, site) %>%
  multi_boot_standard(col = "avg_draw_duration")

num_strokes <- d %>%
  group_by(unique_ids,condition,age_numeric,site) %>%
  summarize(avg_num_strokes = mean(num_strokes)) %>%
  group_by(age_numeric,condition, site) %>%
  multi_boot_standard(col = "avg_num_strokes") 

avg_intensity <- d %>%
  group_by(unique_ids,condition,age_numeric,site) %>%
  summarize(avg_intensity = mean(mean_intensity)) %>%
  group_by(age_numeric,condition, site) %>%
  multi_boot_standard(col = "avg_intensity")
```


```{r plot-effort}
## Make compiled plot of descriptives
base_size_chosen=14 # size of text in plots
smooth_alpha=.2

p1=ggplot(draw_duration, aes(age_numeric,mean, color=condition)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = 0.5)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Draw duration (s)') +
  theme(legend.position = "none") + 
  ylim(0,30) +
  facet_grid(~site)

p2=ggplot(avg_intensity, aes(age_numeric,mean, color=condition)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = 0.5)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Ink used (mean intensity)') +
  theme(legend.position = "none") + 
  ylim(.02,.08) +
  facet_grid(~site)

p3=ggplot(num_strokes, aes(age_numeric,mean, color=condition)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = 0.5)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Number of strokes') +
  theme(legend.position = "none") +
  ylim(0,15) +
  facet_grid(~site)
```

```{r effort-covariates, fig.env="figure", out.width = "\\textwidth", fig.pos = "H", fig.align = "center", fig.width=8, fig.height=3, fig.cap = "Effort covariates measured during the drawing task -- amount of time spent drawing, amount of 'ink' used, and number of strokes used -- as function of the age of the child who completed each drawing, the geographical site they were tested at (San Jose vs. Beijing), and the type of drawing task they completed. Error bars show bootstrapped 95 percent cofnidence intervals."}
cowplot::plot_grid(p1,p2,p3,nrow=1)
```

```{r}
tracing_scores_indiv <- d %>%
  ungroup() %>%
  distinct(unique_ids, avg_tracing_score, site, age_numeric)

tracing_scores <- tracing_scores_indiv %>%
  group_by(age_numeric, site) %>%
  multi_boot_standard(col = 'avg_tracing_score')
```

```{r tracing, fig.env="figure", out.width = "\\textwidth", fig.pos = "H", fig.align = "center", fig.width=8, fig.height=4, fig.cap = "Average tracing scores across age and site; each dot represents an average tracing score obtained for each participant and are slightly jittered. Error bars represent bootstrapped 95 percent confidence intervals."} 
ggplot(tracing_scores, aes(x=age_numeric,y=mean, col=site)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  geom_jitter(data = tracing_scores_indiv, aes(x=age_numeric, y=avg_tracing_score), width=.2, height=.05, alpha=.5) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Tracing score') +
  theme(legend.position = "none") +
  geom_smooth(col='grey', span = 10,alpha=smooth_alpha)  +
  facet_grid(~site) +
  ylim(0,4) +
  scale_color_viridis(discrete=TRUE, name="", option='B', begin=0, end=.4)
```
```{r example-drawings, fig.env = "figure", out.width="100%", fig.pos = "H", fig.label="exampledetections", fig.cap = "Randomly sampled, highly recognized drawings for each task, category, and geographical site made by 6-year-old children."}
knitr::include_graphics("figs/example_drawings.png")
```

```{r}
item_effects_not_spread <- d %>%
  group_by(unique_ids,age_numeric,category, site) %>%
  summarize(avg_sub_cor = mean(prop_correct)) %>%
  group_by(age_numeric,category, site) %>%
  multi_boot_standard(col = "avg_sub_cor")
```

```{r item-effects, fig.env="figure", out.width = "\\textwidth", fig.pos = "H", fig.align = "center", fig.width=8, fig.height=8, fig.cap = "Developmental trajectory of drawing recognizability for each category and for each geographical site. Error bars represent 95 percent bootstrapped confidence intervals."} 
ggplot(item_effects_not_spread, aes(x=as.numeric(age_numeric), y=mean, color=site)) +
  geom_point(alpha=.8) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position=position_dodge(width=.2), alpha=.8) +
  theme_few(base_size = base_size_chosen) + 
  ylim(0,1) + 
  geom_hline(yintercept = 1/12, linetype="dashed", color="grey") +
  ylab('Proportion drawings recognized') +
  xlab('Age (in years)') +
  geom_abline(color = 'grey', intercept=0) +
  geom_smooth(span=20, alpha=.1) +
  facet_wrap(~category) +
  theme(legend.position = 'bottom') +
  scale_color_viridis(discrete=TRUE, name="", option='B', begin=0, end=.4)
```

# Discussion
Here, we examined the consistency and variability in the developmental trajectory of children's drawings across two different drawing tasks and geographical locations. First, we found that asking children to draw from memory ("Can you draw a [cat]?") vs. from perception ("Can you draw this [cat] as it looks in the picture?") had little effect on the recognizability of the drawings that children produced. While older children may have integrated specific visual information that was present in the photographs, in keeping with prior work [@bruner1984], this did not seem to change how recognizable their drawings were to outside observers. Overall, these results suggest that younger children's drawings are unlikely to be constrained by their ability to recall specific, canonical exemplars of object categories when they are trying to draw them. 

Instead, we found that the recogniability of children's drawings differed considerably across  between geographical locations.  Why might this be the case?  Our initial hypothesis was that children who had daily practice with a non-orthographic, complex writing system would show perhaps exhibit greater dexterity in our tracing assessment task.  However, children across the two sites showed equivalent tracing abilites -- despite the fact that children's individual tracing abilities were strong predictors of the recognizability of the drawings that they produced.  A second possibility is that children recruited at preschools near Beijing were more engaged with the task than children recruited at museums near San Jose, USA: this more academic context could have created conditions in which children were incentivized to spend more effort on their drawings. However, our measures of children's effort (e.g., amount of time spent drawing) did not account for differences in recognizability across sites, providing evidence against this account. A final possibility is that children across these two sites may differ in the amount of practice they have drawing common object categories. If so, then children who spend more time drawing may simply have more practice communicating about the diagnostic features of different object categories. In future work, surveys that assesses the amount of time that children spend drawing outside of these experimental contexts could address this hypothesis.

A natural question for future concerns the relationship between the degree to which children across these different geographical sites show differences in their ability to recognize drawings of object categories. In a U.S. based population, prior work has found that older children not only produce more recognizable drawings of object categories but tend to be better skilled at recognizing other children's drawings. Thus, if children in different contexts tend to spend more time practicing drawing, these children may also be more skilled at recognizing drawings of object categories and explicitely identifying the distinctive visual features of these categories.

Furthermore, these results of course do not preclude the possibility that children's drawings differed across these two conditions or geographical sites in other interesting ways beyond their recognizability. Finer-grained measurements of these drawings could reveal interesting differences with respect to the exact visual features that children chose to include, the strategies they employed while drawing, or the order in which they chose to draw certain elements of these categories. 
Furthermore, while this work moves beyond investigating the drawings of children at a single geographical location, these children are still in many ways non-representative of the genera population. These developmental trajectories may thus vary in many ways that are not captured by the slice of experience captured by the present dataset.

Overall, these results take a first step towards characterizing the consistency and variability in the developmental trajectory of children's drawings across development across two different tasks and geographical locations. [more here].

# Acknowledgements  
We thank Yi Feng and Megan Merrick for assistance with data collection, with whom this project would have not been possible. We also gratefully acknowledge the San Jose Children's Discovery Museum for their collaboration. We are also thankful to all of the parents at XX and XX preschool whose children participated. We also thank the members of the Stanford Language and Cognition lab for their feedback. This work was funded by an NSF SPRF-FR Grant \#1714726 to BLL and a Jacobs Foundation Fellowship to MCF. 

\newpage

# References
```{r create_r-references}
r_refs(file = "references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
